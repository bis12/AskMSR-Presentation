<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
  <head>
    <title>AskMSR -- Speaker Notes</title>
    <style type='text/css'>
      /*<![CDATA[*/
        h2 {
          text-align: center;
        }
        body {
          font-size: 200%;
          line-height: 150%;
        }
      /*]]>*/
    </style>
  </head>
  <body>
    <h2>AskMSR -- Speaker Notes</h2>
    <h3>Introduction</h3>
    <ul>
    	<li>Ask Microsoft Research</li>
    	<li>Text retrieval conference</li>
    	<li>this was in the QA track, which no longer exists</li>
    	<li>the problem being solved is identical to IBM&#8217;s Watson.
    	<ul>
    		<li>Given a natural language query, respond with a succinct answer, not a document that contains the answer.</li>
    	</ul></li>
    </ul>
    <hr>
    <h3>Main Result</h3>
    <ul>
    	<li>We can use the web to answer these queries</li>
    	<li>The power in this approach comes from the massive redundancy of data on the web</li>
    	<li>Much easier to do it this way than to find a single answer in a small corpus like the one provided for <span class="caps">TREC</span></li>
    	<li>This system performed as well as the top performers in <span class="caps">TREC</span></li>
    	<li>Got 61% of answers correct</li>
    	<li>Average answer length  was 12 bytes, so the answers were short too</li>
    </ul>
    <hr>
    <h3><span class="caps">MRR</span></h3>
    <ul>
    	<li>Used mean reciprocal rank to find accuracy of AskMSR
    	<ul>
    		<li>multiply 1/#queries by sum of ranks of first correct answer</li>
    	</ul></li>
    </ul>
    <hr>
    <h3>Methodology: Part 1</h3>
    <ul>
    	<li>First, the query is passed through a process that is the main focus of this presentation</li>
    	<li>Next the results are tested against the correct answers provided for <span class="caps">TREC</span></li>
    	<li>problem is that time mismatch between trec and web will create missing answers</li>
    	<li>supporting documents in trec are found as last step</li>
    </ul>
    <hr>
    <h3>Methodology: Part 2</h3>
    <p>Each of these stages makes up a part of the AskMSR system, and will be gone into in depth in later slides.</p>
    <ol>
    	<li>Query Reformulation
    	<ul>
    		<li>The system reformulates query strings into multiple secondary queries to pass on to the search engine</li>
    		<li>The secondary queries are chosen to be substrings of potential answers</li>
    	</ul></li>
    	<li>Search Engine
    	<ul>
    		<li>The search engine used is actually just Google!</li>
    	</ul></li>
    	<li>Mine N-grams
    	<ul>
    		<li>Page summaries from Google results are collected</li>
    		<li>Extract unigram, bigram, and trigrams for later processing</li>
    	</ul></li>
    	<li>Filter N-grams
    	<ul>
    		<li>the n-grams are filtered and reweighted according to how well each candidate matches the expected answer-type</li>
    	</ul></li>
    	<li>Tile N-grams
    	<ul>
    		<li>Finally n-grams are merged into longer and longer answers until none can be combined further</li>
    	</ul></li>
    	<li>Return K best answers
    	<ul>
    		<li>All answers returned with confidence values</li>
    	</ul></li>
    </ol>
    <hr>
    <h3>Implementation:Query Reformulation</h3>
    <ul>
    	<li>Generate successively less and less specific queries.</li>
    	<li>No parser or part-of-speech tagger is used <b>however, do use a lexicon to get morphological variants of some words</b></li>
    	<li>Queries that are more specific are given higher weighting</li>
    </ul>
    <hr>
    <h3>Implementation:Search Engine</h3>
    <ul>
    	<li>Just send the query off to google.</li>
    	<li>Results are taken from the page summaries
    	<ul>
    		<li>not the full page for efficiency&#8217;s sake</li>
    	</ul></li>
    	<li>where is cwru &rarr;  &#8230; Reserve, and <span class="caps">CWRU</span>) is a private research university in Cleveland, Ohio, <span class="caps">USA</span>. &#8230; In U.S. News &amp; World Report&#8217;s 2012 rankings, Case Western Reserve&#8217;s &#8230;</li>
    </ul>
    <hr>
    <h3>Implementation:N-gram Mining</h3>
    <ul>
    	<li>Summary text is broken into unigrams, bigrams, and trigrams</li>
    	<li>scored based on query that supplied this text, based on the metric mentioned before</li>
    	<li>n-gram scores are then summed across how many summaries they showed up in, <b>opposite of idf</b></li>
    	<li>no tf at all</li>
    	<li>from the document: &#8220;the final score for an n-gram is based on the weights associated with the rewrite rules that generated it and the number of unique summaries in which it occurred.&#8221;</li>
    </ul>
    <hr>
    <h3>Implementation:N-gram Filtering</h3>
    <ul>
    	<li>Original query is analyzed, and put into one of 7 bins of &#8220;question type&#8221;</li>
    	<li>based on this, the system has a set of filters to apply for each question (such as presence of digits for a &#8220;when&#8221; question)</li>
    	<li>a rescore is applied based on the results from this stage</li>
    	<li>This is the most important single part of the system. If it is removed, accuracy decreases by 17 percent</li>
    </ul>
    <hr>
    <h3>Implementation:N-gram Tiling</h3>
    <ul>
    	<li>Makes it possible to find answers that are longer than three words</li>
    	<li>substrings don&#8217;t take up multiple slots</li>
    	<li>Works by taking answers that overlap and combining them.  <span class="caps">ABC</span> and <span class="caps">BCD</span> turn into <span class="caps">ABCD</span>.</li>
    	<li>Works greedily with higher scoring candidates first (higher scoring one is turned into longer answer, lower scoring one removed)</li>
    	<li>Iteratively applied until no more combinations can occur</li>
    </ul>
    <hr>
    <h3>Implementation:Redundancy</h3>
    <ul>
    	<li>Redundancy of data allows them to find an answer, even if no clear result is found</li>
    	<li>An example from another paper from these authors is shown here.  They present a query &#8220;How many times did Bjorn Borg win Wimbledon?&#8221;, and show these example results.</li>
    	<li>Because 5 shows up 3 times as often as 37, we can assume that he won it 5 times with a decent degree of certainty.</li>
    	<li>I&#8217;m not sure how much I buy this argument, and I&#8217;ll revisit it later in my critique.</li>
    </ul>
    <hr>
    <h3>Results</h3>
    <ul>
    	<li>As mentioned before, system performed well</li>
    	<li>These are the causes of errors that were present</li>
    	<li>34 percent, (time and correct) were not necessarily problems with AskMSR, just that the trec corpus was off in time, or did not contain the answer at all</li>
    	<li>Completely out of paradigm was the &#8220;number&#8221; problem, where they couldn&#8217;t query for  &#8220;Fiji has &gt;<span class="caps">NUM</span>&lt; islands&#8221;</li>
    	<li>The authors brought up this problem, but I have other critiques too.</li>
    </ul>
    <hr>
    <h3>Critique</h3>
    <ul>
    	<li>Redundancy
    	<ul>
    		<li>The redundancy argument with Bjorn Borg and the Wimbledon seems week to me.</li>
    		<li>Suppose that he won it only once, in 1974 (this was actually his first of 5 wins).</li>
    		<li>Now each article that mentions him and the Wimbledon will most definitely make mention of 1974, and perhaps state that he only won it once.</li>
    		<li>Under the rules for redundancy mentioned before, AskMSR would reply that Bjorn had won 1974 times! Quite a feat, considering the event has only take place since 1877.</li>
    	</ul></li>
    	<li>Query Reformulation
    	<ul>
    		<li>Set of naive rules seems like a weak point of this paper.</li>
    		<li>Although the  authors acknowledge this and mention it as an area for improvement.</li>
    		<li><span class="caps">STOP</span> <span class="caps">WORDS</span> <span class="caps">WERE</span> <span class="caps">ALSO</span> <span class="caps">PROBLEM</span>: <span class="caps">NEW</span> <span class="caps">YORK</span> &#8594; <span class="caps">YORK</span></li>
    	</ul></li>
    </ul>
    <hr>
    <h3>Future Work</h3>
    <ul>
    	<li>May be possible to &#8220;learn&#8221; query reformulation rather than using a small set of naive rules.</li>
    	<li>This would just plain feel better, and probably be better</li>
    	<li>Authors were investigating extending the system to more complex problems (paragaphs of return text)</li>
    </ul>
  </body>
</html>
